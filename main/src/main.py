# -*- coding: utf-8 -*-
"""
T·ªáp m√£ ngu·ªìn t·ªïng h·ª£p cho Nhi·ªám v·ª• 2: Khai ph√° tri th·ª©c t·ª´ vƒÉn b·∫£n k·ªπ thu·∫≠t.

T·ªáp n√†y ch·ª©a to√†n b·ªô pipeline, bao g·ªìm:
1. Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ file PDF sang ƒë·ªãnh d·∫°ng Markdown.
2. L·∫≠p ch·ªâ m·ª•c (index) n·ªôi dung ƒë√£ tr√≠ch xu·∫•t v√†o Vector Database (Qdrant).
3. H·ªá th·ªëng H·ªèi-ƒê√°p (QA) ƒë·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi tr·∫Øc nghi·ªám.
4. T·∫°o t·ªáp output cu·ªëi c√πng (`answer.md` v√† file zip) theo ƒë√∫ng ƒë·ªãnh d·∫°ng y√™u c·∫ßu.

ƒê·ªÉ ch·∫°y ch∆∞∆°ng tr√¨nh, s·ª≠ d·ª•ng command line v·ªõi c√°c t√πy ch·ªçn ph√π h·ª£p.
V√≠ d·ª•:
- Ch·∫°y to√†n b·ªô pipeline cho public test:
  python3 main.py --mode public --task full

- Ch·ªâ ch·∫°y ph·∫ßn tr√≠ch xu·∫•t v√† index cho public test:
  python3 main.py --mode public --task extract

- Ch·ªâ ch·∫°y ph·∫ßn tr·∫£ l·ªùi c√¢u h·ªèi cho public test (sau khi ƒë√£ extract):
  python3 main.py --mode public --task qa
"""

# ==============================================================================
# 0. IMPORTS & SETUP
# ==============================================================================
import os
import re
import sys
import shutil
import argparse
import uuid
import zipfile
from pathlib import Path
import pandas as pd

# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
# pip install "python-dotenv==1.0.1" "pandas==2.2.2" "sentence-transformers==3.0.1" "qdrant-client==1.9.2" "langchain==0.2.6" "langchain-ollama==0.1.0" "jinja2==3.1.4" "docling==0.0.15" "tiktoken==0.7.0"

# T·∫£i .env file n·∫øu c√≥
try:
    from dotenv import load_dotenv
    if Path('.env').exists():
        print("ƒêang t·∫£i bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env...")
        load_dotenv()
except ImportError:
    print("C·∫£nh b√°o: Th∆∞ vi·ªán python-dotenv ch∆∞a ƒë∆∞·ª£c c√†i. S·ª≠ d·ª•ng bi·∫øn m√¥i tr∆∞·ªùng h·ªá th·ªëng.")

# ==============================================================================
# 1. EMBEDDING MODEL (t·ª´ main/src/embedding/model.py)
# ==============================================================================
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    print("L·ªói: sentence-transformers ch∆∞a ƒë∆∞·ª£c c√†i. Vui l√≤ng ch·∫°y: pip install sentence-transformers")
    sys.exit(1)

class DenseEmbedding:
    """Qu·∫£n l√Ω m√¥ h√¨nh embedding ƒë·ªÉ chuy·ªÉn vƒÉn b·∫£n th√†nh vector."""
    def __init__(self, model_name=os.getenv("DENSE_MODEL", "vinai/phobert-base-v2")):
        print(f"ƒêang kh·ªüi t·∫°o m√¥ h√¨nh embedding: {model_name}")
        # T·∫°o th∆∞ m·ª•c cache trong project ƒë·ªÉ l∆∞u model
        cache_dir = Path(__file__).parent / "embedding_models"
        cache_dir.mkdir(exist_ok=True)
        self.model = SentenceTransformer(model_name, cache_folder=str(cache_dir))
        print("‚úÖ Kh·ªüi t·∫°o m√¥ h√¨nh embedding th√†nh c√¥ng.")

    def encode(self, texts):
        if isinstance(texts, str):
            return self.model.encode(texts)
        elif isinstance(texts, list):
            return [e.tolist() for e in self.model.encode(texts)]
        else:
            raise ValueError("ƒê·∫ßu v√†o ph·∫£i l√† chu·ªói ho·∫∑c danh s√°ch c√°c chu·ªói.")
        
    def get_dimension(self):
        # Tr·∫£ v·ªÅ s·ªë chi·ªÅu c·ªßa vector embedding
        return self.model.get_sentence_embedding_dimension()

# ==============================================================================
# 2. VECTOR DATABASE (t·ª´ main/src/vectordb/qdrant.py)
# ==============================================================================
try:
    from qdrant_client import QdrantClient, models
except ImportError:
    print("L·ªói: qdrant-client ch∆∞a ƒë∆∞·ª£c c√†i. Vui l√≤ng ch·∫°y: pip install qdrant-client")
    sys.exit(1)

class VectorStore:
    """Qu·∫£n l√Ω vi·ªác l∆∞u tr·ªØ v√† truy v·∫•n vector t·∫°i Qdrant."""
    def __init__(self, collection_name, dense_model):
        self.collection_name = collection_name
        self.dense_embedding_model = dense_model
        
        try:
            host = os.getenv("QDRANT_HOST", "localhost")
            port = int(os.getenv("QDRANT_PORT", 6333))
            print(f"ƒêang k·∫øt n·ªëi t·ªõi Qdrant t·∫°i {host}:{port}...")
            self.client = QdrantClient(host=host, port=port, timeout=int(os.getenv("QDRANT_TIMEOUT", 60)))
            self.client.get_collections() # Ki·ªÉm tra k·∫øt n·ªëi
            print("‚úÖ K·∫øt n·ªëi Qdrant th√†nh c√¥ng.")
        except Exception as e:
            print(f"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi Qdrant: {e}")
            print("H√£y ch·∫Øc ch·∫Øn r·∫±ng b·∫°n ƒë√£ kh·ªüi ch·∫°y Qdrant (v√≠ d·ª•: b·∫±ng docker-compose).")
            sys.exit(1)

        if not self.client.collection_exists(self.collection_name):
            self._create_collection()

    def _create_collection(self):
        print(f"ƒêang t·∫°o collection m·ªõi: {self.collection_name}")
        self.client.create_collection(
            collection_name=self.collection_name,
            vectors_config=models.VectorParams(
                size=self.dense_embedding_model.get_dimension(),
                distance=models.Distance.COSINE,
            ),
        )

    def recreate_collection(self):
        print(f"ƒêang x√≥a v√† t·∫°o l·∫°i collection: {self.collection_name}")
        if self.client.collection_exists(self.collection_name):
            self.client.delete_collection(self.collection_name)
        self._create_collection()
        print("‚úÖ T·∫°o l·∫°i collection th√†nh c√¥ng.")

    def insert_data(self, payload_keys, payload_values):
        if not payload_values:
            return
        
        contents = [item[0] for item in payload_values]
        dense_embeddings = self.dense_embedding_model.encode(contents)

        points = [
            models.PointStruct(
                id=str(uuid.uuid4()),
                vector=dense_embeddings[i],
                payload=dict(zip(payload_keys, payload_values[i]))
            )
            for i in range(len(dense_embeddings))
        ]
        
        # Upsert theo batch
        BATCH_SIZE = 100
        for i in range(0, len(points), BATCH_SIZE):
            batch = points[i:i + BATCH_SIZE]
            self.client.upsert(collection_name=self.collection_name, points=batch, wait=True)
            print(f"  > ƒê√£ upsert batch {i // BATCH_SIZE + 1}/{len(points) // BATCH_SIZE + 1}")

    def search(self, query, top_k=5, threshold=0.3):
        query_vector = self.dense_embedding_model.encode(query)
        hits = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_vector,
            limit=top_k,
            score_threshold=threshold
        )
        return hits

# ==============================================================================
# 3. LLM INTEGRATION (t·ª´ main/src/llm/*.py)
# ==============================================================================
try:
    from langchain_ollama import OllamaLLM
    from jinja2 import Template
except ImportError:
    print("L·ªói: langchain-ollama ho·∫∑c jinja2 ch∆∞a ƒë∆∞·ª£c c√†i. Vui l√≤ng c√†i ƒë·∫∑t.")
    sys.exit(1)

class LLMManager:
    """Qu·∫£n l√Ω vi·ªác t∆∞∆°ng t√°c v·ªõi Large Language Model."""
    def __init__(self):
        llm_type = os.getenv("LLM_TYPE", "ollama")
        if llm_type != "ollama":
            raise ValueError("Hi·ªán ch·ªâ h·ªó tr·ª£ LLM_TYPE=ollama")
        
        model_name = os.getenv("CHAT_MODEL", "llama3:instruct")
        print(f"ƒêang kh·ªüi t·∫°o LLM: {model_name}")
        self.llm = OllamaLLM(model=model_name, temperature=0.0)
        print("‚úÖ Kh·ªüi t·∫°o LLM th√†nh c√¥ng.")

    def ask(self, prompt):
        try:
            response = self.llm.invoke(prompt)
            return response.strip()
        except Exception as e:
            print(f"‚ùå L·ªói khi g·ªçi LLM: {e}")
            print("H√£y ch·∫Øc ch·∫Øn r·∫±ng Ollama ƒëang ch·∫°y v√† model ƒë√£ ƒë∆∞·ª£c pull (vd: ollama run llama3:instruct).")
            # Fallback response
            return "S·ªë c√¢u ƒë√∫ng: 1\nƒê√°p √°n ƒë√∫ng: A"

# ==============================================================================
# 4. PDF EXTRACTION (t·ª´ extract_pdf.py)
# ==============================================================================
try:
    from docling.document_converter import DocumentConverter, PdfFormatOption
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode
except ImportError:
    print("L·ªói: docling ch∆∞a ƒë∆∞·ª£c c√†i. Vui l√≤ng ch·∫°y: pip install docling")
    sys.exit(1)

class PDFExtractor:
    """Tr√≠ch xu·∫•t n·ªôi dung t·ª´ t·ªáp PDF sang Markdown."""
    def __init__(self):
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_table_structure = True
        pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE
        
        self.converter = DocumentConverter(
            format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}
        )
    
    def extract_pdf(self, pdf_path, output_dir):
        output_dir_path = Path(output_dir)
        output_dir_path.mkdir(parents=True, exist_ok=True)
        images_dir = output_dir_path / "images"
        images_dir.mkdir(exist_ok=True)
        
        print(f"ƒêang x·ª≠ l√Ω PDF: {pdf_path}")
        result = self.converter.convert(str(pdf_path))
        doc = result.document
        
        # L∆∞u h√¨nh ·∫£nh v√† c√¥ng th·ª©c (n·∫øu c√≥)
        # Theo y√™u c·∫ßu, ch√∫ng ta ch·ªâ c·∫ßn placeholder
        if hasattr(doc, 'pictures'):
            for i, picture in enumerate(doc.pictures):
                try:
                    if hasattr(picture, 'image') and picture.image:
                        img_path = images_dir / f"image_{i+1}.png"
                        picture.image.pil_image.save(img_path)
                except Exception as e:
                    print(f"  > C·∫£nh b√°o: Kh√¥ng th·ªÉ l∆∞u ·∫£nh {i+1}. L·ªói: {e}")

        # Export ra markdown v√† thay th·∫ø placeholders
        md_content = doc.export_to_markdown()
        md_content = re.sub(r'!\[.*?\]\(data:image/.*?\)', r'|<image_placeholder>|', md_content)
        md_content = re.sub(r'<img src="data:image/.*?">', r'|<image_placeholder>|', md_content)
        
        # ƒê√°nh s·ªë l·∫°i placeholders
        image_counter = 1
        formula_counter = 1
        
        def replace_image(match):
            nonlocal image_counter
            res = f"|<image_{image_counter}>|"
            image_counter += 1
            return res

        def replace_formula(match):
            nonlocal formula_counter
            res = f"|<formula_{formula_counter}>|"
            formula_counter += 1
            return res
            
        md_content = re.sub(r'\|<image_placeholder>\|', replace_image, md_content)
        md_content = re.sub(r'\$\$[\s\S]*?\$\$', replace_formula, md_content) # Block formula
        md_content = re.sub(r'\$[^\$]*?\$', replace_formula, md_content) # Inline formula

        main_md_path = output_dir_path / "main.md"
        with open(main_md_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        print(f"‚úÖ ƒê√£ tr√≠ch xu·∫•t xong: {main_md_path}")
        return md_content

    def extract_all_pdfs(self, input_dir, output_base_dir):
        extracted_data = {}
        pdf_files = list(Path(input_dir).glob("*.pdf"))
        
        if not pdf_files:
            print(f"C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file PDF n√†o trong '{input_dir}'")
            return {}

        for pdf_path in pdf_files:
            pdf_name = pdf_path.stem
            output_dir = Path(output_base_dir) / pdf_name
            markdown_content = self.extract_pdf(pdf_path, output_dir)
            extracted_data[pdf_name] = markdown_content
        return extracted_data

# ==============================================================================
# 5. QA SYSTEM (t·ª´ qa_system.py v√† _utils.py)
# ==============================================================================
try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
except ImportError:
    print("L·ªói: langchain ch∆∞a ƒë∆∞·ª£c c√†i. Vui l√≤ng ch·∫°y: pip install langchain tiktoken")
    sys.exit(1)

def chunking(text):
    """Ph√¢n nh·ªè vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n c√≥ k√≠ch th∆∞·ªõc ph√π h·ª£p."""
    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=512, chunk_overlap=100
    )
    chunks = text_splitter.split_text(text)
    return [chunk for chunk in chunks if len(chunk.strip()) >= 50]

class QASystem:
    """H·ªá th·ªëng tr·∫£ l·ªùi c√¢u h·ªèi tr·∫Øc nghi·ªám."""
    def __init__(self, vector_store, llm_manager):
        self.vector_store = vector_store
        self.llm_manager = llm_manager
    
    def index_data(self, extracted_data):
        print("üîÑ ƒêang index d·ªØ li·ªáu v√†o vector database...")
        self.vector_store.recreate_collection()
        
        all_chunks = []
        for pdf_name, content in extracted_data.items():
            chunks = chunking(content)
            for chunk in chunks:
                all_chunks.append([chunk, pdf_name])
        
        if all_chunks:
            self.vector_store.insert_data(["content", "source"], all_chunks)
        
        print(f"‚úÖ ƒê√£ index {len(all_chunks)} chunks t·ª´ {len(extracted_data)} PDF.")
    
    def answer_question(self, question, options):
        # T√¨m ki·∫øm context li√™n quan
        search_results = self.vector_store.search(question, top_k=5, threshold=0.3)
        
        context = "\n\n---\n\n".join([
            f"Ngu·ªìn: {point.payload.get('source', 'Kh√¥ng r√µ')}\n\n{point.payload.get('content', '')}"
            for point in search_results
        ]) if search_results else "Kh√¥ng c√≥ th√¥ng tin n√†o ƒë∆∞·ª£c t√¨m th·∫•y trong t√†i li·ªáu."
        
        prompt = self._create_qa_prompt(question, options, context)
        response = self.llm_manager.ask(prompt)
        return self._parse_llm_response(response)
    
    def _create_qa_prompt(self, question, options, context):
        options_text = "\n".join([f"{key}. {value}" for key, value in options.items()])
        
        return f"""B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch t√†i li·ªáu k·ªπ thu·∫≠t. D·ª±a v√†o "TH√îNG TIN T√ÄI LI·ªÜU" d∆∞·ªõi ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi tr·∫Øc nghi·ªám m·ªôt c√°ch ch√≠nh x√°c.

### TH√îNG TIN T√ÄI LI·ªÜU:
{context}

---

### C√ÇU H·ªéI:
{question}

### C√ÅC L·ª∞A CH·ªåN:
{options_text}

### Y√äU C·∫¶U:
1. ƒê·ªçc k·ªπ c√¢u h·ªèi v√† t·∫•t c·∫£ c√°c l·ª±a ch·ªçn.
2. ƒê·ªëi chi·∫øu T·ª™NG l·ª±a ch·ªçn v·ªõi "TH√îNG TIN T√ÄI LI·ªÜU".
3. C√¢u h·ªèi c√≥ th·ªÉ c√≥ M·ªòT ho·∫∑c NHI·ªÄU ƒë√°p √°n ƒë√∫ng.
4. Ch·ªâ ch·ªçn nh·ªØng ƒë√°p √°n ƒë∆∞·ª£c x√°c nh·∫≠n HO√ÄN TO√ÄN b·ªüi t√†i li·ªáu.
5. Tr·∫£ l·ªùi theo ƒë·ªãnh d·∫°ng JSON nghi√™m ng·∫∑t sau ƒë√¢y, kh√¥ng th√™m b·∫•t k·ª≥ gi·∫£i th√≠ch n√†o kh√°c.

{{
  "correct_count": <s·ªë l∆∞·ª£ng ƒë√°p √°n ƒë√∫ng>,
  "correct_answers": ["<A>", "<B>", ...]
}}

V√≠ d·ª•:
{{
  "correct_count": 2,
  "correct_answers": ["A", "C"]
}}

### TR·∫¢ L·ªúI (CH·ªà JSON):
"""

    def _parse_llm_response(self, response):
        try:
            import json
            # T√¨m v√† tr√≠ch xu·∫•t chu·ªói JSON t·ª´ response
            match = re.search(r'\{[\s\S]*\}', response)
            if match:
                json_str = match.group(0)
                data = json.loads(json_str)
                count = data.get("correct_count", 0)
                answers = data.get("correct_answers", [])
                
                # X√°c th·ª±c l·∫°i d·ªØ li·ªáu
                answers = [ans for ans in answers if ans in ['A', 'B', 'C', 'D']]
                if count != len(answers):
                    print(f"  > C·∫£nh b√°o: LLM tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng kh√¥ng kh·ªõp. count={count}, answers={answers}. T·ª± ƒë·ªông s·ª≠a l·∫°i.")
                    count = len(answers)
                
                return count, answers
            else:
                raise ValueError("Kh√¥ng t√¨m th·∫•y JSON trong response.")
        except (json.JSONDecodeError, ValueError) as e:
            print(f"  > C·∫£nh b√°o: Kh√¥ng th·ªÉ parse JSON t·ª´ LLM. L·ªói: {e}. Response: '{response[:100]}...'")
            # Fallback: D√πng regex ƒë·ªÉ t√¨m c√¢u tr·∫£ l·ªùi
            answers = sorted(list(set(re.findall(r'\b([A-D])\b', response.upper()))))
            return len(answers), answers
    
    def process_questions_csv(self, csv_path):
        try:
            df = pd.read_csv(csv_path)
        except FileNotFoundError:
            print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file question.csv t·∫°i '{csv_path}'")
            return None
            
        results = []
        print(f"\nü§î B·∫Øt ƒë·∫ßu tr·∫£ l·ªùi {len(df)} c√¢u h·ªèi...")
        
        for idx, row in df.iterrows():
            question = row.iloc[0]
            options = { 'A': row.iloc[1], 'B': row.iloc[2], 'C': row.iloc[3], 'D': row.iloc[4] }
            
            print(f"\nC√¢u {idx + 1}/{len(df)}: {question[:80]}...")
            
            count, answers = self.answer_question(question, options)
            results.append((count, answers))
            
            print(f"  ‚ûú K·∫øt qu·∫£: {count} c√¢u ƒë√∫ng - ƒê√°p √°n: {', '.join(answers) if answers else 'Kh√¥ng c√≥'}")
        
        return results

# ==============================================================================
# 6. ANSWER GENERATOR (t·ª´ answer_generator.py)
# ==============================================================================
class AnswerGenerator:
    """T·∫°o file output cu·ªëi c√πng theo ƒë·ªãnh d·∫°ng c·ªßa cu·ªôc thi."""
    def __init__(self, output_dir):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_answer_md(self, extracted_data, qa_results):
        content = []
        
        # Ph·∫ßn 1: TASK EXTRACT
        content.append("### TASK EXTRACT")
        for pdf_name, md_content in sorted(extracted_data.items()):
            content.append(f"\n# {pdf_name}\n")
            content.append(md_content)
        
        # Ph·∫ßn 2: TASK QA
        content.append("\n### TASK QA\n")
        if qa_results:
            for count, answers in qa_results:
                content.append(f"{count}")
                content.append(f"{','.join(sorted(answers))}")
        
        answer_md_path = self.output_dir / "answer.md"
        with open(answer_md_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(content))
        
        print(f"‚úÖ ƒê√£ t·∫°o file answer.md t·∫°i: {answer_md_path}")

    def create_zip(self, zip_name):
        zip_path = self.output_dir.parent / zip_name
        print(f"üì¶ ƒêang t·∫°o file zip: {zip_path}...")
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, _, files in os.walk(self.output_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    archive_name = os.path.relpath(file_path, self.output_dir)
                    zipf.write(file_path, archive_name)

        print(f"‚úÖ ƒê√£ t·∫°o file zip th√†nh c√¥ng: {zip_path}")

# ==============================================================================
# 7. MAIN PIPELINE LOGIC
# ==============================================================================
def setup_paths(mode):
    """Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n input v√† output d·ª±a tr√™n mode."""
    base_input_dir = Path(f"data/{mode}_test_input")
    output_dir = Path(f"output/{mode}_test_output")
    
    # T·∫°o c√°c th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
    base_input_dir.mkdir(parents=True, exist_ok=True)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    paths = {
        "pdf_dir": base_input_dir,
        "question_csv": base_input_dir / "question.csv",
        "output_dir": output_dir,
        "zip_name": f"{mode}_test_output.zip"
    }
    
    print("\n--- C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n ---")
    for key, value in paths.items():
        print(f"{key:<15}: {value}")
    print("---------------------------\n")
    
    return paths

def run_task_extract(paths):
    """Ch·∫°y t√°c v·ª• tr√≠ch xu·∫•t v√† index."""
    print("\n" + "="*25 + " B·∫ÆT ƒê·∫¶U T√ÅC V·ª§ EXTRACT " + "="*25)
    
    # 1. Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn
    extractor = PDFExtractor()
    embedding_model = DenseEmbedding()
    vector_db = VectorStore(f"collection_{Path(paths['pdf_dir']).name}", embedding_model)
    qa_system = QASystem(vector_db, None) # Kh√¥ng c·∫ßn LLM cho task n√†y

    # 2. Tr√≠ch xu·∫•t PDF
    extracted_data = extractor.extract_all_pdfs(paths["pdf_dir"], paths["output_dir"])
    if not extracted_data:
        print("‚ùå K·∫øt th√∫c t√°c v·ª• extract v√¨ kh√¥ng c√≥ d·ªØ li·ªáu PDF.")
        return False

    # 3. Index d·ªØ li·ªáu
    qa_system.index_data(extracted_data)
    
    print("\n" + "="*25 + " HO√ÄN TH√ÄNH T√ÅC V·ª§ EXTRACT " + "="*24)
    return True

def run_task_qa(paths):
    """Ch·∫°y t√°c v·ª• tr·∫£ l·ªùi c√¢u h·ªèi v√† t·∫°o output."""
    print("\n" + "="*28 + " B·∫ÆT ƒê·∫¶U T√ÅC V·ª§ QA " + "="*28)
    
    # 1. Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn
    embedding_model = DenseEmbedding()
    vector_db = VectorStore(f"collection_{Path(paths['pdf_dir']).name}", embedding_model)
    llm = LLMManager()
    qa_system = QASystem(vector_db, llm)

    # 2. ƒê·ªçc l·∫°i d·ªØ li·ªáu ƒë√£ tr√≠ch xu·∫•t t·ª´ file main.md
    extracted_data = {}
    output_dir_path = Path(paths["output_dir"])
    for subdir in output_dir_path.iterdir():
        if subdir.is_dir():
            main_md = subdir / "main.md"
            if main_md.exists():
                pdf_name = subdir.name
                extracted_data[pdf_name] = main_md.read_text(encoding='utf-8')

    if not extracted_data:
        print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ƒë√£ tr√≠ch xu·∫•t trong th∆∞ m·ª•c output.")
        print("Vui l√≤ng ch·∫°y t√°c v·ª• 'extract' tr∆∞·ªõc: python3 main.py --task extract")
        return

    # 3. Tr·∫£ l·ªùi c√¢u h·ªèi
    qa_results = qa_system.process_questions_csv(paths["question_csv"])
    if qa_results is None:
        print("‚ùå K·∫øt th√∫c t√°c v·ª• QA v√¨ kh√¥ng th·ªÉ x·ª≠ l√Ω file c√¢u h·ªèi.")
        return

    # 4. T·∫°o file output
    generator = AnswerGenerator(paths["output_dir"])
    generator.generate_answer_md(extracted_data, qa_results)
    
    # 5. Copy file main.py v√† t·∫°o zip
    print(f"ƒêang copy file m√£ ngu·ªìn '{Path(__file__).name}' v√†o th∆∞ m·ª•c output...")
    shutil.copy(Path(__file__), output_dir_path / "main.py")
    generator.create_zip(paths["zip_name"])
    
    print("\n" + "="*27 + " HO√ÄN TH√ÄNH T√ÅC V·ª§ QA " + "="*28)


def main():
    """H√†m main ƒë·ªÉ ch·∫°y pipeline t·ª´ command line."""
    parser = argparse.ArgumentParser(description="Pipeline cho Nhi·ªám v·ª• 2 - Zalo AI Challenge")
    
    parser.add_argument(
        "--mode", type=str, choices=["public", "private", "training"], default="public",
        help="Ch·∫ø ƒë·ªô ch·∫°y: public, private ho·∫∑c training test."
    )
    parser.add_argument(
        "--task", type=str, choices=["extract", "qa", "full"], default="full",
        help="T√°c v·ª• c·∫ßn th·ª±c hi·ªán: extract (tr√≠ch xu·∫•t & index), qa (tr·∫£ l·ªùi c√¢u h·ªèi), full (to√†n b·ªô pipeline)."
    )
    
    args = parser.parse_args()
    
    print("\n" + "*"*80)
    print(f" B·∫ÆT ƒê·∫¶U PIPELINE - CH·∫æ ƒê·ªò: {args.mode.upper()} - T√ÅC V·ª§: {args.task.upper()} ".center(80, '*'))
    print("*"*80)

    # Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n
    paths = setup_paths(args.mode)

    if args.task == "extract":
        run_task_extract(paths)
    elif args.task == "qa":
        run_task_qa(paths)
    elif args.task == "full":
        if run_task_extract(paths):
            run_task_qa(paths)
    
    print("\n" + "*"*80)
    print(f" PIPELINE K·∫æT TH√öC ".center(80, '*'))
    print("*"*80 + "\n")


if __name__ == "__main__":
    # ƒê·∫∑t th∆∞ m·ª•c l√†m vi·ªác l√† th∆∞ m·ª•c ch·ª©a file script
    os.chdir(Path(__file__).parent)
    main()
