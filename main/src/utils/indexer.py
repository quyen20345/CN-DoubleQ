from main.src.utils._utils import get_files_in_directory, load_file_as_markdown
from main.src.utils.collections import COLLECTIONS
from main.src.utils._utils import chunking
from main.src.vectordb.qdrant import VectorStore

def preprocess_file(path):
    chunked_data = []

    # X·ª≠ l√Ω file .docx, .txt v√† .pdf
    if path.endswith(".docx") or path.endswith(".pdf") or path.endswith(".txt"):
        print(f"Processing file: {path}")
        file_name = path.split("/")[-1]
        markdown_text = load_file_as_markdown(path)
        chunk_contents = chunking(markdown_text)
        chunked_data = [[chunk, file_name] for chunk in chunk_contents]
    else:
        pass

    return chunked_data

def load_and_index_data(vector_store, path):
    vector_store.recreate_collection()

    print(f"Indexing data from {path}...")
    files = get_files_in_directory(path)
    
    for path in files:
        chunked_data = preprocess_file(path)
        vector_store.insert_data(["content", "source"], chunked_data, [0, 1])


def index_extracted_data(extracted_data: dict, vector_store: VectorStore):
    """
    Chunk v√† index d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t v√†o vector database.
    """
    print("üîÑ ƒêang chunk v√† index d·ªØ li·ªáu...")
    vector_store.recreate_collection()
    
    all_chunks_with_source = []
    for pdf_name, content in extracted_data.items():
        chunks = chunking(content)
        for chunk in chunks:
            # Payload bao g·ªìm n·ªôi dung chunk v√† t√™n file ngu·ªìn
            all_chunks_with_source.append([chunk, pdf_name])
    
    if all_chunks_with_source:
        # Keys c·ªßa payload ph·∫£i kh·ªõp v·ªõi l√∫c insert
        vector_store.insert_data(["content", "source"], all_chunks_with_source)
    
    print(f"‚úÖ ƒê√£ index {len(all_chunks_with_source)} chunks t·ª´ {len(extracted_data)} PDF.")
    