# -*- coding: utf-8 -*-
import re
from typing import List
from main.src.llm.llm_integrations import get_llm

# =================================================================
# ‚úÖ C√ÅC THAM S·ªê T·ªêI ∆ØU
# =================================================================

# Ng∆∞·ª°ng ƒë·ªÉ g·ªôp c√°c ƒëi·ªÉm ng·∫Øt g·∫ßn nhau (s·ªë c√¢u)
DEBOUNCE_THRESHOLD = 2
# Ng∆∞·ª°ng s·ªë t·ª´ t·ªëi thi·ªÉu cho m·ªôt chunk
MIN_CHUNK_WORDS = 40

def call_ollama_for_chunking(prompt: str) -> str | None:
    """
    Calls the configured Ollama LLM and returns the response.
    This function is adapted to use the project's LLM integration.
    """
    try:
        llm = get_llm(temperature=0.0)
        # We set a low max_tokens for this specific task, as we only expect a number.
        # Note: Langchain Ollama wrapper might not directly support num_predict.
        # The model behavior should still be constrained by the prompt.
        response = llm.invoke(prompt)
        return response.strip()
    except Exception as e:
        print(f"\n‚ö†Ô∏è L·ªói khi g·ªçi API Ollama: {e}")
        return None

def post_process_chunks(chunks: list[str], min_words: int) -> list[str]:
    """
    H·∫≠u x·ª≠ l√Ω c√°c chunk: g·ªôp c√°c chunk qu√° nh·ªè v√†o chunk tr∆∞·ªõc ƒë√≥.
    """
    print(f"\n‚öôÔ∏è H·∫≠u x·ª≠ l√Ω: G·ªôp c√°c chunk c√≥ √≠t h∆°n {min_words} t·ª´...")
    processed_chunks = []
    for chunk in chunks:
        if not processed_chunks:
            processed_chunks.append(chunk)
            continue

        # N·∫øu chunk hi·ªán t·∫°i qu√° nh·ªè, g·ªôp n√≥ v√†o chunk tr∆∞·ªõc ƒë√≥
        if len(chunk.split()) < min_words:
            print(f"  - Merging small chunk (words: {len(chunk.split())}) into previous one.")
            processed_chunks[-1] += " " + chunk
        else:
            processed_chunks.append(chunk)
    return processed_chunks

def advanced_llm_chunker(
    text: str,
    window_size: int = 15,
    step_size: int = 5
) -> list[str]:
    """
    Phi√™n b·∫£n chunker n√¢ng cao v·ªõi logic l·ªçc ƒëi·ªÉm ng·∫Øt v√† h·∫≠u x·ª≠ l√Ω chunk.
    S·ª≠ d·ª•ng LLM ƒë∆∞·ª£c c·∫•u h√¨nh trong d·ª± √°n.
    """
    text_cleaned = re.sub(r'\s+', ' ', text)
    sentences = re.split(r'(?<=[.!?])\s+', text_cleaned)
    sentences = [s.strip() for s in sentences if s.strip()]

    if not sentences:
        return []

    split_indices = set()
    print(f"\nüöÄ B·∫Øt ƒë·∫ßu ph√¢n t√≠ch {len(sentences)} c√¢u b·∫±ng c·ª≠a s·ªï tr∆∞·ª£t...")

    for i in range(0, len(sentences), step_size):
        window_end = min(i + window_size, len(sentences))
        window_sentences = sentences[i:window_end]

        if len(window_sentences) < 5: continue

        numbered_sentences_str = "\n".join(
            f"{idx + 1}. {sent}" for idx, sent in enumerate(window_sentences)
        )
        prompt = f"""You are an expert text segmentation engine. Your task is to find the single most significant topic change in the following numbered list of sentences. A good split point is usually a heading or the start of a new, distinct topic.

Analyze the text below. Return ONLY the integer number of the sentence where the main topic changes. For example, if the topic changes at sentence 5, your response must be exactly "5". Do not add any explanation, punctuation, or text.

TEXT TO ANALYZE:
---
{numbered_sentences_str}
---

The number of the best sentence to split at is:"""

        response_text = call_ollama_for_chunking(prompt)
        if not response_text: continue

        try:
            extracted_numbers = re.findall(r'\d+', response_text)
            if not extracted_numbers: continue

            split_num_in_window = int(extracted_numbers[0])
            if 1 < split_num_in_window < len(window_sentences):
                global_index = i + split_num_in_window - 1
                split_indices.add(global_index)
                print(f"  - üß© C·ª≠a s·ªï [{i}-{window_end}]: ƒê·ªÅ xu·∫•t ng·∫Øt t·∫°i c√¢u to√†n c·ª•c s·ªë {global_index}")
        except (ValueError, IndexError):
            print(f"  - ‚ö†Ô∏è C·ª≠a s·ªï [{i}-{window_end}]: Ph·∫£n h·ªìi kh√¥ng h·ª£p l·ªá: '{response_text}'")

    # ================================================================
    # T·ªêI ∆ØU 1: L·ªåC V√Ä G·ªòP C√ÅC ƒêI·ªÇM NG·∫ÆT G·∫¶N NHAU (DEBOUNCING)
    # ================================================================
    print(f"\n‚öôÔ∏è L·ªçc ƒëi·ªÉm ng·∫Øt: G·ªôp c√°c ƒëi·ªÉm c√°ch nhau d∆∞·ªõi {DEBOUNCE_THRESHOLD} c√¢u...")
    if not split_indices:
        print("  - ‚ùï Kh√¥ng t√¨m th·∫•y ƒëi·ªÉm ng·∫Øt n√†o. Tr·∫£ v·ªÅ to√†n b·ªô vƒÉn b·∫£n.")
        return [text]

    sorted_indices = sorted(list(split_indices))
    debounced_indices = []
    if sorted_indices:
        last_index = sorted_indices[0]
        debounced_indices.append(last_index)
        for index in sorted_indices[1:]:
            if index - last_index > DEBOUNCE_THRESHOLD:
                debounced_indices.append(index)
                last_index = index
            else:
                print(f"  - Ignoring split at {index} (too close to {last_index})")

    # ================================================================
    # T·ªêI ∆ØU 2: S·ª¨A LOGIC C·∫ÆT CHUNK
    # ================================================================
    print("\nüì¶ ƒêang gh√©p l·∫°i th√†nh c√°c ƒëo·∫°n cu·ªëi c√πng v·ªõi logic m·ªõi...")
    final_chunks = []
    start_idx = 0
    for split_point in debounced_indices:
        chunk_sentences = sentences[start_idx:split_point]
        if chunk_sentences:
            final_chunks.append(" ".join(chunk_sentences))
        start_idx = split_point

    last_chunk_sentences = sentences[start_idx:]
    if last_chunk_sentences:
        final_chunks.append(" ".join(last_chunk_sentences))

    # ================================================================
    # T·ªêI ∆ØU 3: H·∫¨U X·ª¨ L√ù G·ªòP CHUNK QU√Å NH·ªé
    # ================================================================
    final_chunks = post_process_chunks(final_chunks, min_words=MIN_CHUNK_WORDS)

    return [chunk for chunk in final_chunks if chunk.strip()]
