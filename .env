# ===================================

# CHỌN MÔI TRƯỜNG LLM

# ===================================

# LLM_TYPE có thể là "ollama" (chạy local) hoặc "colab" (chạy từ xa).

LLM_TYPE=ollama

# --- Cấu hình cho Ollama (nếu LLM_TYPE=ollama) ---

CHAT_MODEL=qwen2.5:3b

# Các model khác < 4B bạn có thể dùng:

# - phi3:3.8b

# - gemma2:2b

# - stablelm2:1.6b

# --- Cấu hình cho Colab (nếu LLM_TYPE=colab) ---

# Dán URL API của Colab/ngrok vào đây

# COLAB_API_URL=http://your-ngrok-or-colab-url-goes-here/api/generate

# ===================================

# Embedding Model - Tối ưu cho tiếng Việt

# ===================================

# Bỏ comment model bạn muốn sử dụng.

DENSE_MODEL=intfloat/multilingual-e5-base

# DENSE_MODEL=keepitreal/vietnamese-sbert

# DENSE_MODEL=BAAI/bge-m3

# ===================================

# Vector Database (Qdrant) Configuration

# ===================================

QDRANT_HOST=localhost
QDRANT_PORT=6333

# Tăng timeout để cho phép các tác vụ lớn (như indexing) có đủ thời gian

QDRANT_TIMEOUT=600

# ===================================

# Chunking Strategy - Chọn phương pháp chunking

# ===================================

# Thay đổi giá trị này để thử nghiệm các chiến lược khác nhau.

# Các lựa chọn: recursive_char, token, semantic_similarity, llm_window, propositional

CHUNKING_STRATEGY=recursive_char

# ===================================

# Search Settings - Tối ưu cho độ chính xác

# ===================================

TOP_K=8
SIMILARITY_THRESHOLD=0.25

# ===================================

# LLM Settings

# ===================================

# Temperature thấp cho câu trả lời nhất quán, logic

TEMPERATURE=0.0

# Giới hạn token tối đa cho context gửi đến LLM

MAX_CONTEXT_TOKENS=3000

# ===================================

# Advanced RAG Settings

# ===================================

USE_RERANKING=true
USE_MULTI_QUERY=true
NUM_QUERY_VARIANTS=2