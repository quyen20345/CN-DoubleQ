# src/vectordb/indexer.py
"""
Module nÃ y chá»©a logic Ä‘á»ƒ chunking, embedding, vÃ  táº£i dá»¯ liá»‡u vÃ o Qdrant.
"""

import uuid
import os
import json
from dotenv import load_dotenv
from typing import Dict, List, Generator, Tuple
from qdrant_client.models import PointStruct

from .store import VectorStore
from src.chunking import get_chunking_strategy

load_dotenv()

def _batch_generator(data: List, batch_size: int) -> Generator[List, None, None]:
    """Táº¡o ra cÃ¡c khá»‘i (batches) dá»¯ liá»‡u tá»« má»™t danh sÃ¡ch."""
    for i in range(0, len(data), batch_size):
        yield data[i:i + batch_size]

def index_documents(extracted_data: Dict[str, str], vector_store: VectorStore) -> List[Dict[str, str]]:
    """
    Xá»­ lÃ½ vÃ  index dá»¯ liá»‡u, Ä‘á»“ng thá»i tráº£ vá» corpus cho BM25.
    
    Returns:
        List[Dict[str, str]]: Corpus chá»©a táº¥t cáº£ cÃ¡c chunk Ä‘á»ƒ sá»­ dá»¥ng cho BM25.
    """
    print("ğŸ”„ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh chunking vÃ  indexing...")
    
    chunking_strategy_name = os.getenv("CHUNKING_STRATEGY", "recursive_char")
    chunk_text = get_chunking_strategy(chunking_strategy_name)
    
    vector_store.recreate_collection()
    
    all_points = []
    corpus_for_bm25 = []
    total_chunks = 0
    
    for doc_name, content in extracted_data.items():
        print(f"  - Äang xá»­ lÃ½ tÃ i liá»‡u: {doc_name}")
        
        raw_chunks = chunk_text(content)
        if not raw_chunks:
            print(f"    - âš ï¸ KhÃ´ng táº¡o Ä‘Æ°á»£c chunk nÃ o cho {doc_name}.")
            continue
            
        chunks = [f"[{doc_name}] {chunk}" for chunk in raw_chunks]
        
        embeddings = vector_store.embedding_model.encode(chunks)
        
        for chunk, emb in zip(chunks, embeddings):
            all_points.append(
                PointStruct(
                    id=str(uuid.uuid4()),
                    vector=emb,
                    payload={"content": chunk, "source": doc_name}
                )
            )
            corpus_for_bm25.append({"content": chunk, "source": doc_name})
        
        print(f"    - ÄÃ£ táº¡o {len(chunks)} chunks.")
        total_chunks += len(chunks)

    if all_points:
        BATCH_SIZE = 128
        print(f"\nğŸš€ Äang táº£i {len(all_points)} Ä‘iá»ƒm dá»¯ liá»‡u lÃªn Qdrant theo tá»«ng khá»‘i {BATCH_SIZE} Ä‘iá»ƒm...")
        
        for batch in _batch_generator(all_points, BATCH_SIZE):
            vector_store.client.upsert(
                collection_name=vector_store.collection_name,
                points=batch,
                wait=True
            )
            print(f"   - ÄÃ£ táº£i lÃªn thÃ nh cÃ´ng {len(batch)} Ä‘iá»ƒm.")
    
    print(f"âœ… HoÃ n thÃ nh indexing! Tá»•ng cá»™ng {total_chunks} chunks.")
    return corpus_for_bm25

